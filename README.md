# SweetEscape ğŸ¬

SweetEscape ist ein Machine-Learning-Projekt zur Klassifikation des Diabetes-Risikos
(**kein Diabetes**, **PrÃ¤diabetes**, **Diabetes**) auf Basis von Gesundheits- und Lebensstilmerkmalen
aus dem BRFSS-2015-Datensatz.

Ziel ist eine nachvollziehbare ML-Pipeline (Daten â†’ Feature Engineering â†’ Modellvergleich â†’ finales Modell â†’ Web-App).

---

## Projektstruktur

- `data/raw/` â€“ originale Rohdaten (unverÃ¤ndert)
- `data/processed/` â€“ verarbeitete Daten (Feature Engineering Output)
- `notebooks/` â€“ nachvollziehbarer Workflow in zwei Schritten
  - `00_preprocessing_feature_engineering.ipynb` â€“ Preprocessing & Feature Engineering, schreibt `diabetes_fe.csv`
  - `01_train_model.ipynb` â€“ Training, Vergleich von Modellen, Auswahl & Speichern des finalen Modells
- `models/` â€“ gespeichertes finales Modell (`diabetes_final_model.joblib`)
- `src/` â€“ wiederverwendbarer Projektcode (z. B. Pfade/Konstanten, FE-Funktionen)
- `app.py` â€“ Streamlit-Webanwendung (lokal ausfÃ¼hrbar)

**Warum diese Struktur?**  
Sie trennt Rohdaten, Verarbeitung, Training und Deployment sauber und macht das Projekt reproduzierbar und prÃ¼fbar.

---

## Workflow (Reproduzierbarkeit)

1. **Rohdaten** liegen in `data/raw/diabetes_raw.csv`
2. Notebook **00** ausfÃ¼hren  
   â†’ erzeugt `data/processed/diabetes_fe.csv` (moderates Feature Engineering)
3. Notebook **01** ausfÃ¼hren  
   â†’ trainiert und vergleicht **3 Modelle** auf identischem Train/Test-Split  
   â†’ speichert das **finale Modell** als `models/diabetes_final_model.joblib`
4. `app.py` lokal starten  
   â†’ lÃ¤dt das gespeicherte Modell und macht Vorhersagen fÃ¼r Nutzer-Eingaben

---

## Feature Engineering (Notebook 00)

Der Datensatz ist bereits stark vorverarbeitet (viele 0/1-Indikatoren und kategoriale Codes).
Daher wird bewusst **nur moderates Feature Engineering** durchgefÃ¼hrt, um Interpretierbarkeit zu erhalten.

Beispiele fÃ¼r abgeleitete Features:
- `inactive` (aus `PhysActivity`)
- `cardio_risk_sum` (Summenfeature aus kardiovaskulÃ¤ren Risikoindikatoren)
- `lifestyle_risk_sum` (Summenfeature aus Lifestyle-Risiken)
- `poor_health` (binÃ¤r aus `GenHlth`)
- `mental_physical_burden` (Summenfeature aus `MentHlth` + `PhysHlth`)

Output: `data/processed/diabetes_fe.csv`

---

## Modelltraining & Vergleich (Notebook 01)

Es werden drei Modelle trainiert und fair verglichen (gleicher Split, gleiche Metriken):

1. **Logistic Regression** (`class_weight="balanced"`) â€“ interpretierbare Baseline
2. **Random Forest** (`class_weight="balanced_subsample"`) â€“ nicht-linearer Vergleich
3. **HistGradientBoosting** (mit `sample_weight`) â€“ Boosting-Ansatz fÃ¼r tabellarische Daten

### Warum Macro-F1 als Hauptmetrik?

Der Datensatz ist stark unausgeglichen (viele FÃ¤lle â€kein Diabetesâ€œ, sehr wenige â€PrÃ¤diabetesâ€œ).
Accuracy wÃ¤re daher irrefÃ¼hrend, weil ein Modell durch Vorhersage der Mehrheitsklasse bereits hoch abschneiden kann.

**Macro-F1** mittelt den F1-Score **Ã¼ber alle Klassen**, sodass Minderheitsklassen (PrÃ¤diabetes/Diabetes)
gleichwertig berÃ¼cksichtigt werden. ZusÃ¤tzlich werden **Recall der Klassen 1 und 2** sowie die **Confusion Matrix**
betrachtet, um kritische Fehlklassifikationen sichtbar zu machen.

### Ergebnis & finale Modellwahl

Basierend auf dem Vergleich wurde **Logistic Regression (balanced)** als finales Modell gewÃ¤hlt,
da es die beste Balance Ã¼ber alle Klassen (hÃ¶chster Macro-F1) liefert und gut interpretierbar bleibt.

Gespeichert als:
- `models/diabetes_final_model.joblib`

---

## Web-App (lokal)

Die Web-Anwendung ist mit **Streamlit** umgesetzt und lÃ¤dt das gespeicherte Modell.

Start (im Projekt-Root, venv aktiv):
```bash
streamlit run app.py
